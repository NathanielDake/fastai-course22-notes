{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "d = 48\n",
    "\n",
    "X = torch.randn((N, d))\n",
    "w = torch.randn((d, 1))\n",
    "b = torch.randn(1)\n",
    "y = torch.randn((N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1]), torch.Size([1000, 1]), tensor(0.), tensor([14.4401]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Pass\n",
    "z = X @ w + b\n",
    "a = z.clamp_min(0)\n",
    "mse_loss = (a - y).pow(2).mean(dim=0) \n",
    "\n",
    "z.shape, a.shape, a.min(), mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* See this [blog post](https://nasheqlbrm.github.io/blog/posts/2021-11-13-backward-pass.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.w + self.b\n",
    "\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, a):\n",
    "        return a.clamp_min(0)\n",
    "\n",
    "\n",
    "class MSE():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, target, pred):\n",
    "        return (target - pred).pow(2).mean(dim=0)\n",
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, w, b):\n",
    "        self.layers = [\n",
    "            Linear(w, b),\n",
    "            Relu()\n",
    "        ]\n",
    "        self.loss = MSE()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.loss(y, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the forward Pass is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.4401])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No wrapping via Model\n",
    "linear_layer1 = Linear(w, b)\n",
    "z = linear_layer1(X)\n",
    "a = Relu()(z)\n",
    "MSE()(y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.4401])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapping via Model\n",
    "model = Model(w, b)\n",
    "model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient via pytorch so we have a baseline to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.w + self.b\n",
    "\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, a):\n",
    "        a.retain_grad()\n",
    "        self.a = a\n",
    "        return a.clamp_min(0)\n",
    "\n",
    "\n",
    "class MSE():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, target, pred):\n",
    "        pred.retain_grad()\n",
    "        self.target = target\n",
    "        self.pred = pred\n",
    "        return (target - pred).pow(2).mean(dim=0)\n",
    "\n",
    "\n",
    "class Model():\n",
    "\n",
    "    def __init__(self, w, b):\n",
    "        self.layers = [\n",
    "            Linear(w, b),\n",
    "            Relu()\n",
    "        ]\n",
    "        self.loss = MSE()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.loss(y, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): \n",
    "    return x.clone().requires_grad_(True)\n",
    "\n",
    "chks = w, b, X\n",
    "ptgrads = w_prime, b_prime, X_prime = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prime = Model(w_prime, b_prime)\n",
    "loss_prime = model_prime(X_prime, y)\n",
    "loss_prime.backward()\n",
    "\n",
    "# w_prime.grad, b_prime.grad, model_prime.loss.pred.grad[0:10], model_prime.layers[-1].a.grad[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update classes to compute gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = x @ self.w + self.b\n",
    "        return self.output\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # Will be used for chain rule\n",
    "        dloss_doutput = self.output.g\n",
    "\n",
    "        # Gradient of loss wrt w\n",
    "        X = self.input\n",
    "        doutput_dw = X.T\n",
    "        self.w.g = doutput_dw @ dloss_doutput\n",
    " \n",
    "        # Gradient of loss wrt b\n",
    "        doutput_db = 1\n",
    "        self.b.g = (dloss_doutput * doutput_db).sum(dim=0)\n",
    "\n",
    "\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.input = x\n",
    "        self.output = x.clamp_min(0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        dloss_doutput = self.output.g\n",
    "        doutput_dinput = (self.input > 0).float()\n",
    "        # Chain rule - Element wise multiplication\n",
    "        self.input.g = dloss_doutput * doutput_dinput \n",
    "\n",
    "\n",
    "class MSE():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, target, pred):\n",
    "        self.input = pred\n",
    "        self.target = target\n",
    "        self.out = (target - pred).pow(2).mean(dim=0)\n",
    "        return self.out \n",
    "    \n",
    "    def backward(self):\n",
    "        N = self.target.shape[0]\n",
    "        pred = self.input\n",
    "        target = self.target\n",
    "        # No chain rule - this is a leaf node\n",
    "        dloss_dpred = (2 / N) * (pred - target)\n",
    "        self.input.g = dloss_dpred\n",
    "\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, w, b):\n",
    "        self.layers = [\n",
    "            Linear(w, b),\n",
    "            Relu()\n",
    "        ]\n",
    "        self.loss = MSE()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.loss(y, x)\n",
    "    \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.4401])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapping via Model\n",
    "model = Model(w, b)\n",
    "loss = model(X, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss.backward()\n",
    "model.layers[-1].backward()\n",
    "model.layers[-2].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss wrt pred\n",
    "test_close(model.loss.input.g[0:10], model_prime.loss.pred.grad[0:10])\n",
    "# Gradient of loss wrt input to relu\n",
    "test_close(model.layers[-1].input.g[0:10], model_prime.layers[-1].a.grad[0:10])\n",
    "# Gradient of loss wrt w\n",
    "test_close(model.layers[-2].w.g[0:10], model_prime.layers[-2].w.grad[0:10])\n",
    "# Gradient of loss wrt b\n",
    "test_close(model.layers[-2].b.g[0:10], model_prime.layers[-2].b.grad[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor: Add forward method, backward to model, use a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.input = args\n",
    "        self.output = self.forward(*args)\n",
    "        return self.output\n",
    "    \n",
    "    def forward(self):\n",
    "        raise Exception('Not Implemented')\n",
    "    \n",
    "    def backward(self):\n",
    "        self.bwd(self.output, *self.input)\n",
    "    \n",
    "    def bwd(self):\n",
    "        raise Exception('Not Implemented')\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b\n",
    "\n",
    "    def bwd(self, output, input_):\n",
    "\n",
    "        # Will be used for chain rule\n",
    "        dloss_doutput = output.g\n",
    "\n",
    "        # Gradient of loss wrt w\n",
    "        X = input_\n",
    "        doutput_dw = X.T\n",
    "        self.w.g = doutput_dw @ dloss_doutput\n",
    " \n",
    "        # Gradient of loss wrt b\n",
    "        doutput_db = 1\n",
    "        self.b.g = (dloss_doutput * doutput_db).sum(dim=0)\n",
    "\n",
    "\n",
    "class Relu(Module):\n",
    "    def forward(self, x):\n",
    "        return x.clamp_min(0)\n",
    "    \n",
    "    def bwd(self, output, input_):\n",
    "        dloss_doutput = output.g\n",
    "        doutput_dinput = (input_ > 0).float()\n",
    "        # Chain rule - Element wise multiplication\n",
    "        input_.g = dloss_doutput * doutput_dinput \n",
    "\n",
    "\n",
    "class MSE(Module):    \n",
    "    def forward(self, target, pred):\n",
    "        return (target - pred).pow(2).mean(dim=0)\n",
    "    \n",
    "    def bwd(self, output, target, input_):\n",
    "        N = target.shape[0]\n",
    "        # No chain rule - this is a leaf node\n",
    "        dloss_dinput = (2 / N) * (input_ - target)\n",
    "        input_.g = dloss_dinput\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, w, b):\n",
    "        self.layers = [\n",
    "            Linear(w, b),\n",
    "            Relu()\n",
    "        ]\n",
    "        self.loss = MSE()\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.loss(y, x)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.4401])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapping via Model\n",
    "model = Model(w, b)\n",
    "loss = model(X, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of loss wrt w\n",
    "test_close(w.g[0:10], model_prime.layers[-2].w.grad[0:10])\n",
    "# Gradient of loss wrt b\n",
    "test_close(b.g[0:10], model_prime.layers[-2].b.grad[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
